{"cells":[{"cell_type":"markdown","id":"fecb2fb5-b87f-4428-8175-e3a46fe77371","metadata":{"id":"fecb2fb5-b87f-4428-8175-e3a46fe77371"},"source":["## Tutorial: Optimizing a Prompt\n","\n","![TextGrad](https://github.com/vinid/data/blob/master/logo_full.png?raw=true)\n","\n","An autograd engine -- for textual gradients!\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zou-group/TextGrad/blob/main/examples/notebooks/Prompt-Optimization.ipynb)\n","[![GitHub license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)\n","[![Arxiv](https://img.shields.io/badge/arXiv-2406.07496-B31B1B.svg)](https://arxiv.org/abs/2406.07496)\n","[![Documentation Status](https://readthedocs.org/projects/textgrad/badge/?version=latest)](https://textgrad.readthedocs.io/en/latest/?badge=latest)\n","[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/textgrad)](https://pypi.org/project/textgrad/)\n","[![PyPI](https://img.shields.io/pypi/v/textgrad)](https://pypi.org/project/textgrad/)\n","\n","**Objectives:**\n","\n","* In this tutorial, we will run prompt optimization.\n","\n","**Requirements:**\n","\n","* You need to have an OpenAI API key to run this tutorial. This should be set as an environment variable as OPENAI_API_KEY.\n"]},{"cell_type":"code","source":["#@title Python Dependencies\n","!uv pip install -qU textgrad --system # you might need to restart the notebook after installing textgrad"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yl9ZFyit1A-L","executionInfo":{"status":"ok","timestamp":1737737212342,"user_tz":-60,"elapsed":250200,"user":{"displayName":"Luis Arias","userId":"12702625469088953108"}},"outputId":"1b902868-8dd5-4a8e-a00c-8f83353524b4"},"id":"Yl9ZFyit1A-L","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting textgrad\n","  Downloading textgrad-0.1.6.tar.gz (71 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/72.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting datasets<2.0.0\n","  Downloading datasets-1.18.4-py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: openai>=1.23.6 in /usr/local/lib/python3.11/dist-packages (from textgrad) (1.59.6)\n","Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from textgrad) (9.0.0)\n","Collecting python-dotenv>=1.0.0 (from textgrad)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: pandas>=1.5.3 in /usr/local/lib/python3.11/dist-packages (from textgrad) (2.2.2)\n","Requirement already satisfied: platformdirs>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from textgrad) (4.3.6)\n","INFO: pip is looking at multiple versions of textgrad to determine which version is compatible with other requirements. This could take a while.\n","Collecting textgrad\n","  Downloading textgrad-0.1.5.tar.gz (65 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Downloading textgrad-0.1.4.tar.gz (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m361.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Downloading textgrad-0.1.3.tar.gz (37 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: Cannot install datasets<2.0.0, textgrad==0.1.3, textgrad==0.1.4, textgrad==0.1.5 and textgrad==0.1.6 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n","\u001b[0m\n","The conflict is caused by:\n","    The user requested datasets<2.0.0\n","    textgrad 0.1.6 depends on datasets>=2.14.6\n","    The user requested datasets<2.0.0\n","    textgrad 0.1.5 depends on datasets>=2.14.6\n","    The user requested datasets<2.0.0\n","    textgrad 0.1.4 depends on datasets>=2.14.6\n","    The user requested datasets<2.0.0\n","    textgrad 0.1.3 depends on datasets>=2.14.6\n","\n","To fix this you could try to:\n","1. loosen the range of package versions you've specified\n","2. remove package versions to allow pip to attempt to solve the dependency conflict\n","\n","\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n","\u001b[0mCollecting git+https://github.com/google/BIG-bench.git\n","  Cloning https://github.com/google/BIG-bench.git to /tmp/pip-req-build-27p4lc82\n","  Running command git clone --filter=blob:none --quiet https://github.com/google/BIG-bench.git /tmp/pip-req-build-27p4lc82\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n","    status = run_func(*args)\n","             ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n","    return func(self, options, args)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n","    requirement_set = resolver.resolve(\n","                      ^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 76, in resolve\n","    collected = self.factory.collect_root_requirements(root_reqs)\n","                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 538, in collect_root_requirements\n","    reqs = list(\n","           ^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 494, in _make_requirements_from_install_req\n","    cand = self._make_base_candidate_from_link(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 231, in _make_base_candidate_from_link\n","    self._link_candidate_cache[link] = LinkCandidate(\n","                                       ^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 303, in __init__\n","    super().__init__(\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 158, in __init__\n","    self.dist = self._prepare()\n","                ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 235, in _prepare\n","    dist = self._prepare_distribution()\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 314, in _prepare_distribution\n","    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/prepare.py\", line 527, in prepare_linked_requirement\n","    return self._prepare_linked_requirement(req, parallel_builds)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/prepare.py\", line 598, in _prepare_linked_requirement\n","    local_file = unpack_url(\n","                 ^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/prepare.py\", line 159, in unpack_url\n","    unpack_vcs_link(link, location, verbosity=verbosity)\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/prepare.py\", line 81, in unpack_vcs_link\n","    vcs_backend.unpack(location, url=hide_url(link.url), verbosity=verbosity)\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/vcs/versioncontrol.py\", line 589, in unpack\n","    self.obtain(location, url=url, verbosity=verbosity)\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/vcs/versioncontrol.py\", line 502, in obtain\n","    self.fetch_new(dest, url, rev_options, verbosity=verbosity)\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/vcs/git.py\", line 277, in fetch_new\n","    self.run_command(\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/vcs/versioncontrol.py\", line 631, in run_command\n","    return call_subprocess(\n","           ^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/subprocess.py\", line 151, in call_subprocess\n","    line: str = proc.stdout.readline()\n","                ^^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/pip3\", line 8, in <module>\n","    sys.exit(main())\n","             ^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n","    return command.main(cmd_args)\n","           ^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n","    return self._main(args)\n","           ^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n","    return run(options, args)\n","           ^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n","    logger.critical(\"Operation cancelled by user\")\n","  File \"/usr/lib/python3.11/logging/__init__.py\", line 1526, in critical\n","    def critical(self, msg, *args, **kwargs):\n","\n","KeyboardInterrupt\n","^C\n"]}]},{"cell_type":"code","source":["#@title Setup Secrets\n","import os\n","\n","from google.colab import userdata\n","\n","os.environ[\"OPENAI_API_KEY\"] = userdata.get('ALBERT_API_KEY')\n","os.environ[\"OPENAI_API_BASE\"] = \"https://albert.api.etalab.gouv.fr/v1\""],"metadata":{"id":"p1zodaYK1HE1","executionInfo":{"status":"ok","timestamp":1737736673336,"user_tz":-60,"elapsed":1138,"user":{"displayName":"Luis Arias","userId":"12702625469088953108"}}},"id":"p1zodaYK1HE1","execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"id":"7add4547-4278-411b-a827-79be521851f1","metadata":{"ExecuteTime":{"end_time":"2024-06-11T19:30:34.029594610Z","start_time":"2024-06-11T19:30:34.024175489Z"},"id":"7add4547-4278-411b-a827-79be521851f1","outputId":"46749ff7-c43c-4495-e80b-c82386789169","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737736684024,"user_tz":-60,"elapsed":8000,"user":{"displayName":"Luis Arias","userId":"12702625469088953108"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n","* 'fields' has been removed\n","  warnings.warn(message, UserWarning)\n"]}],"source":["#@title Python Imports\n","\n","import argparse\n","import concurrent\n","from tqdm import tqdm\n","import textgrad as tg\n","from tg.tasks import load_task\n","import numpy as np\n","import random\n"]},{"cell_type":"markdown","id":"9a459a37-7446-4c4a-a7e0-38182b5dbd3e","metadata":{"id":"9a459a37-7446-4c4a-a7e0-38182b5dbd3e"},"source":["Let's first define some support functions"]},{"cell_type":"code","execution_count":3,"id":"1ccc3b21bf9ddc48","metadata":{"ExecuteTime":{"end_time":"2024-06-11T19:30:42.098338405Z","start_time":"2024-06-11T19:30:42.093473103Z"},"id":"1ccc3b21bf9ddc48","executionInfo":{"status":"ok","timestamp":1737736687982,"user_tz":-60,"elapsed":216,"user":{"displayName":"Luis Arias","userId":"12702625469088953108"}}},"outputs":[],"source":["def set_seed(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)"]},{"cell_type":"code","execution_count":4,"id":"649e06aef34d0990","metadata":{"id":"649e06aef34d0990","executionInfo":{"status":"ok","timestamp":1737736689309,"user_tz":-60,"elapsed":219,"user":{"displayName":"Luis Arias","userId":"12702625469088953108"}}},"outputs":[],"source":["def eval_sample(item, eval_fn, model):\n","    \"\"\"\n","    This function allows us to evaluate if an answer to a question in the prompt is a good answer.\n","\n","    \"\"\"\n","    x, y = item\n","    x = tg.Variable(x, requires_grad=False, role_description=\"query to the language model\")\n","    y = tg.Variable(y, requires_grad=False, role_description=\"correct answer for the query\")\n","    response = model(x)\n","    try:\n","        eval_output_variable = eval_fn(inputs=dict(prediction=response, ground_truth_answer=y))\n","        return int(eval_output_variable.value)\n","    except:\n","        eval_output_variable = eval_fn([x, y, response])\n","        eval_output_parsed = eval_fn.parse_output(eval_output_variable)\n","        return int(eval_output_parsed)"]},{"cell_type":"code","execution_count":5,"id":"9559a31e07e54d7f","metadata":{"id":"9559a31e07e54d7f","executionInfo":{"status":"ok","timestamp":1737736691879,"user_tz":-60,"elapsed":224,"user":{"displayName":"Luis Arias","userId":"12702625469088953108"}}},"outputs":[],"source":["def eval_dataset(test_set, eval_fn, model, max_samples: int=None):\n","    if max_samples is None:\n","        max_samples = len(test_set)\n","    accuracy_list = []\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n","        futures = []\n","        for _, sample in enumerate(test_set):\n","\n","            future = executor.submit(eval_sample, sample, eval_fn, model)\n","            futures.append(future)\n","            if len(futures) >= max_samples:\n","                break\n","        tqdm_loader = tqdm(concurrent.futures.as_completed(futures), total=len(futures), position=0)\n","        for future in tqdm_loader:\n","            acc_item = future.result()\n","            accuracy_list.append(acc_item)\n","            tqdm_loader.set_description(f\"Accuracy: {np.mean(accuracy_list)}\")\n","    return accuracy_list"]},{"cell_type":"code","execution_count":6,"id":"4ea732b7edf34eb9","metadata":{"id":"4ea732b7edf34eb9","executionInfo":{"status":"ok","timestamp":1737736694560,"user_tz":-60,"elapsed":211,"user":{"displayName":"Luis Arias","userId":"12702625469088953108"}}},"outputs":[],"source":["def run_validation_revert(system_prompt: tg.Variable, results, model, eval_fn, val_set):\n","    val_performance = np.mean(eval_dataset(val_set, eval_fn, model))\n","    previous_performance = np.mean(results[\"validation_acc\"][-1])\n","    print(\"val_performance: \", val_performance)\n","    print(\"previous_performance: \", previous_performance)\n","    previous_prompt = results[\"prompt\"][-1]\n","\n","    if val_performance < previous_performance:\n","        print(f\"rejected prompt: {system_prompt.value}\")\n","        system_prompt.set_value(previous_prompt)\n","        val_performance = previous_performance\n","\n","    results[\"validation_acc\"].append(val_performance)"]},{"cell_type":"code","execution_count":8,"id":"e69f8431-661c-42f8-b7fc-efccea588a03","metadata":{"id":"e69f8431-661c-42f8-b7fc-efccea588a03","outputId":"70c5b7cc-2569-443c-ec8d-8743c03edb10","colab":{"base_uri":"https://localhost:8080/","height":698,"referenced_widgets":["b1cdd06b1a1e4413a7c6495d26bc8fe9","fa2ae6c3c1404138b47129e57e5569d5","e2fb94b74a8e450faa62eb123d787ff7","ac47320c1a9b4e0088143b4567c9e6b9","33c930d93c4b44c5bf11e1bb42ca1111","6fcc8a2d9da74bc7a40ac095a476f91f","abee7f06559c456ba10aa9af5eca45f7","778cf62cd9a6423391b18d2b263c8fb3","e7e15ae3b5f84658b88fef91be2179ea","c8579ea06a5844248591a5b2a68c645d","75139cbb0c0e467c825ebc6fda0c6c14","de0b99a5bcad47f3a3f96b354d75182c","17d8ce8b72bd4ecc917c5f94bc26855a","d01bef9191e54155bc1d0f4a25d84671","e0671a1ce02e48bc81eff4ea25676742","2fece52c9bea46bf9672f499fab4d815","4316ad07dbe642748b9e741fe03b3e70","f4a960b954fc4b36a663d5d8ff692342","00ca0ccc8b5c44d28465bd2917d0a46f","08c80bd2ee6d4e818670fd90fc848dc6","b60ab69a03a54e6f8b1c784bf7cae130","31500b0fa96d48ad90636adda0a416f4"]},"executionInfo":{"status":"error","timestamp":1737736710457,"user_tz":-60,"elapsed":9828,"user":{"displayName":"Luis Arias","userId":"12702625469088953108"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/99.8k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1cdd06b1a1e4413a7c6495d26bc8fe9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["bigbench.py:   0%|          | 0.00/20.5k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de0b99a5bcad47f3a3f96b354d75182c"}},"metadata":{}},{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'load_metric' from 'datasets' (/usr/local/lib/python3.11/dist-packages/datasets/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-19d2e3bf9829>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the data and the evaluation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BBH_object_counting\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation_api\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm_api_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train/Val/Test Set Lengths: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mSTARTING_SYSTEM_PROMPT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_task_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-d8442c1c8fec>\u001b[0m in \u001b[0;36mload_task\u001b[0;34m(task_name, evaluation_api, *args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Load the dataset using the 'datasets' library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bigbench\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Access the train, validation, and test splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2129\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2130\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1884\u001b[0;31m     \u001b[0mbuilder_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset_builder_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1885\u001b[0m     \u001b[0;31m# Instantiate the dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1886\u001b[0m     builder_instance: DatasetBuilder = builder_cls(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mget_dataset_builder_class\u001b[0;34m(dataset_module, dataset_name)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     ):\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuilder_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_main_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdataset_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder_configs_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder_configs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdataset_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset_name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mimport_main_class\u001b[0;34m(module_path)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimport_main_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;34m\"\"\"Import a module at module_path and return its main class: a DatasetBuilder\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;31m# Find the main class in our imported module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0mmodule_main_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/bigbench/d2757373c3fb6b35a846ee951265c3f8fbf0124fb650b12cef5678cf902914d2/bigbench.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbigbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbb_utils\u001b[0m  \u001b[0;31m# From: \"bigbench @ https://storage.googleapis.com/public_research_data/bigbench/bigbench-0.0.1.tar.gz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbigbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbseqio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigbench_bridge\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbbb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson_task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bigbench/api/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbigbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtask_api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbigbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_task\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbigbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodel_api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbigbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresults_api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bigbench/api/json_task.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbigbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbigbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_metrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbigbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresults_api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bigbench/api/task_metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mt5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'load_metric' from 'datasets' (/usr/local/lib/python3.11/dist-packages/datasets/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["set_seed(12)\n","llm_api_eval = tg.get_engine(engine_name=\"experimental:openai/neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8\")\n","llm_api_test = tg.get_engine(engine_name=\"experimental:openai/google/gemma-2-9b-it\")\n","tg.set_backward_engine(llm_api_eval, override=True)\n","\n","# Load the data and the evaluation function\n","train_set, val_set, test_set, eval_fn = load_task(\"BBH_object_counting\", evaluation_api=llm_api_eval)\n","print(\"Train/Val/Test Set Lengths: \", len(train_set), len(val_set), len(test_set))\n","STARTING_SYSTEM_PROMPT = train_set.get_task_description()\n"]},{"cell_type":"markdown","id":"f40b576c-4ba0-4e6e-b3ed-81eb44524676","metadata":{"id":"f40b576c-4ba0-4e6e-b3ed-81eb44524676"},"source":["This is the system prompt we are going to start from:"]},{"cell_type":"code","execution_count":null,"id":"d3ed3261-6f9d-4906-8c4b-a3ad570f5950","metadata":{"id":"d3ed3261-6f9d-4906-8c4b-a3ad570f5950"},"outputs":[],"source":["print(STARTING_SYSTEM_PROMPT)\n"]},{"cell_type":"code","execution_count":null,"id":"f7544127-38e0-4c74-8632-003efcc645ee","metadata":{"id":"f7544127-38e0-4c74-8632-003efcc645ee","outputId":"c4db06f6-7168-4338-e98e-62688b9b5702"},"outputs":[{"name":"stderr","output_type":"stream","text":["Accuracy: 0.7142857142857143: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1438.83it/s]\n","Accuracy: 0.6867469879518072: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:00<00:00, 1318.42it/s]\n"]}],"source":["train_loader = tg.tasks.DataLoader(train_set, batch_size=3, shuffle=True)\n","\n","\n","# Testing the 0-shot performance of the evaluation engine\n","system_prompt = tg.Variable(STARTING_SYSTEM_PROMPT,\n","                            requires_grad=True,\n","                            role_description=\"system prompt to the language model\")\n","model_evaluation = tg.BlackboxLLM(llm_api_eval, system_prompt)\n","\n","system_prompt = tg.Variable(STARTING_SYSTEM_PROMPT,\n","                            requires_grad=True,\n","                            role_description=\"structured system prompt to a somewhat capable language model that specifies the behavior and strategies for the QA task\")\n","model = tg.BlackboxLLM(llm_api_test, system_prompt)\n","\n","optimizer = tg.TextualGradientDescent(engine=llm_api_eval, parameters=[system_prompt])\n","\n","results = {\"test_acc\": [], \"prompt\": [], \"validation_acc\": []}\n","results[\"test_acc\"].append(eval_dataset(test_set, eval_fn, model))\n","results[\"validation_acc\"].append(eval_dataset(val_set, eval_fn, model))\n","results[\"prompt\"].append(system_prompt.get_value())\n"]},{"cell_type":"code","execution_count":null,"id":"b3807736-1d81-4349-95db-257c20110d1a","metadata":{"id":"b3807736-1d81-4349-95db-257c20110d1a","outputId":"7bb4e01b-bc27-450f-b240-c89209135534"},"outputs":[{"name":"stderr","output_type":"stream","text":["Accuracy: 0.5542168674698795: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:21<00:00,  3.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["val_performance:  0.5542168674698795\n","previous_performance:  0.6867469879518072\n","rejected prompt: You will answer a reasoning question. Provide the final numerical answer directly. Your response should be a single numerical value. Do not include any explanation or additional text, only the numerical answer.\n","sys prompt:  You will answer a reasoning question. Think step by step. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value.\n"]},{"name":"stderr","output_type":"stream","text":["Accuracy: 0.7142857142857143: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1201.24it/s]\n","Accuracy: 0.8313253012048193: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:43<00:00,  1.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["val_performance:  0.8313253012048193\n","previous_performance:  0.6867469879518072\n","sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of the question and focus on counting the items accurately. List each item you count and then verify the total number. Avoid adding any extra information or context that is not directly related to the total count. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"]},{"name":"stderr","output_type":"stream","text":["Accuracy: 0.8214285714285714: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:43<00:00,  1.94it/s]\n","Accuracy: 0.5542168674698795: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:46<00:00,  1.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["val_performance:  0.5542168674698795\n","previous_performance:  0.8313253012048193\n","rejected prompt: You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of the question and focus on counting the items accurately. \n","\n","1. Restate the question to ensure clarity.\n","2. List each item you count on a new line using bullet points for clarity. If there are multiple items of the same type, list them together and indicate the quantity.\n","3. Ensure the numbering and items in the list are consistent and free from typographical errors.\n","4. After listing all items, count the total number of items and provide the final count in the specified format.\n","5. Verify the total number by cross-checking with the list.\n","6. If there are potential errors or ambiguities in the list, acknowledge them and request additional details if necessary.\n","\n","The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n","sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of the question and focus on counting the items accurately. List each item you count and then verify the total number. Avoid adding any extra information or context that is not directly related to the total count. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"]},{"name":"stderr","output_type":"stream","text":["Accuracy: 0.8214285714285714: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1638.96it/s]\n","Accuracy: 0.6867469879518072: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:18<00:00,  4.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["val_performance:  0.6867469879518072\n","previous_performance:  0.8313253012048193\n","rejected prompt: You will answer a reasoning question that involves counting items in a list. Directly compute the total number of items and provide the final count. Ensure you understand the context of the question and focus on counting the items accurately. Avoid adding any extra information or context that is not directly related to the total count. Ensure your response follows the format 'Answer: $VALUE' with no additional text. If the input contains unexpected characters or is malformed, correct the input and provide a coherent response. Ensure the final line contains only the answer in the required format.\n","sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of the question and focus on counting the items accurately. List each item you count and then verify the total number. Avoid adding any extra information or context that is not directly related to the total count. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"]},{"name":"stderr","output_type":"stream","text":["Accuracy: 0.8214285714285714: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1043.66it/s]\n","Training step 3. Epoch 0: : 3it [07:29, 149.70s/it]\n","Accuracy: 0.4819277108433735: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:34<00:00,  2.44it/s]\n"]},{"name":"stdout","output_type":"stream","text":["val_performance:  0.4819277108433735\n","previous_performance:  0.8313253012048193\n","rejected prompt: Answer a reasoning question that involves counting items in a list. Focus on counting only the items relevant to the question. Directly provide the total count without listing each item. Ensure the final line contains only the answer in the format 'Answer: $VALUE' where VALUE is a numerical value. Double-check your count for accuracy before providing the final number. If you encounter any ambiguous items or quantities, make a note of them and proceed with the calculation based on the clear items.\n","sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of the question and focus on counting the items accurately. List each item you count and then verify the total number. Avoid adding any extra information or context that is not directly related to the total count. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"]},{"name":"stderr","output_type":"stream","text":["Accuracy: 0.8214285714285714: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1660.49it/s]\n","Accuracy: 0.3132530120481928: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:16<00:00,  5.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["val_performance:  0.3132530120481928\n","previous_performance:  0.8313253012048193\n","rejected prompt: You will answer a reasoning question that involves counting items in a list. Ensure you understand the context of the question, such as identifying specific categories of items (e.g., vegetables) before counting. Count the items and provide the total number. Be cautious of items that might be similar or easily confused, and ensure you are counting the correct items based on the context. If the query is ambiguous or could be interpreted in multiple ways, provide a brief explanation of your reasoning or ask for clarification. Ensure the response is a single numerical value without any additional text or formatting.\n","sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of the question and focus on counting the items accurately. List each item you count and then verify the total number. Avoid adding any extra information or context that is not directly related to the total count. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"]},{"name":"stderr","output_type":"stream","text":["Accuracy: 0.8214285714285714: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1674.21it/s]\n","Accuracy: 0.891566265060241: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:00<00:00,  1.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["val_performance:  0.891566265060241\n","previous_performance:  0.8313253012048193\n","sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of each item and its relevance to the total count. Use bullet points or numbering with periods for listing items. Maintain consistent naming conventions for similar items (e.g., Bed 1, Bed 2). If there is any ambiguity, provide reasoning for your choice or ask a clarifying question. After listing the items, verify the total count to ensure accuracy. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"]},{"name":"stderr","output_type":"stream","text":["Accuracy: 0.8571428571428571: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [01:00<00:00,  1.38it/s]\n","Accuracy: 0.8313253012048193: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:43<00:00,  1.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["val_performance:  0.8313253012048193\n","previous_performance:  0.891566265060241\n","rejected prompt: You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of each item and its relevance to the total count. Use bullet points with a hyphen (-) for each item. Maintain consistent naming conventions for similar items (e.g., Bed 1, Bed 2). If there is any ambiguity, provide reasoning for your choice or ask a clarifying question. After listing the items, state the total number of items in the format: 'The total number of items listed is X.' Ensure the final line contains only the answer in the required format: 'Answer: $VALUE' where VALUE is a numerical value. Double-check your arithmetic to ensure the sum of the counts is correct.\n","sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of each item and its relevance to the total count. Use bullet points or numbering with periods for listing items. Maintain consistent naming conventions for similar items (e.g., Bed 1, Bed 2). If there is any ambiguity, provide reasoning for your choice or ask a clarifying question. After listing the items, verify the total count to ensure accuracy. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"]},{"name":"stderr","output_type":"stream","text":["Accuracy: 0.8571428571428571: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1432.86it/s]\n","Training step 3. Epoch 1: : 3it [08:16, 165.36s/it]\n","Accuracy: 0.6144578313253012: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:18<00:00,  4.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["val_performance:  0.6144578313253012\n","previous_performance:  0.891566265060241\n","rejected prompt: You will answer a reasoning question that involves counting items in a list. Provide only the final count of items without listing intermediate steps. Ensure you understand the context of each item and its relevance to the total count. If there is any ambiguity, ask a clarifying question. Present the numerical value clearly and directly, without any surrounding text or context. Ensure the final answer is a single numerical value without any additional text. Do not repeat the final answer; provide it only once in the specified format. Avoid using ellipses, parentheses, or any other punctuation that could complicate the response. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n","sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of each item and its relevance to the total count. Use bullet points or numbering with periods for listing items. Maintain consistent naming conventions for similar items (e.g., Bed 1, Bed 2). If there is any ambiguity, provide reasoning for your choice or ask a clarifying question. After listing the items, verify the total count to ensure accuracy. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"]},{"name":"stderr","output_type":"stream","text":["Accuracy: 0.8571428571428571: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1608.54it/s]\n","Accuracy: 0.5301204819277109: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:16<00:00,  5.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["val_performance:  0.5301204819277109\n","previous_performance:  0.891566265060241\n","rejected prompt: Count the items and provide the total number as a single integer. Do not include any additional text, explanations, or lists in your response. Ensure the answer is a numerical value only.\n","sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of each item and its relevance to the total count. Use bullet points or numbering with periods for listing items. Maintain consistent naming conventions for similar items (e.g., Bed 1, Bed 2). If there is any ambiguity, provide reasoning for your choice or ask a clarifying question. After listing the items, verify the total count to ensure accuracy. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"]},{"name":"stderr","output_type":"stream","text":["Accuracy: 0.8571428571428571: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1517.28it/s]\n","Accuracy: 0.8433734939759037: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:20<00:00,  1.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["val_performance:  0.8433734939759037\n","previous_performance:  0.891566265060241\n","rejected prompt: You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of each item and its relevance to the total count. Use bullet points or numbering with periods for listing items. Maintain consistent naming conventions for similar items (e.g., Bed 1, Bed 2). Ensure that the numbering is sequential and does not skip any numbers. Avoid using ellipses or incomplete names; list each item explicitly. If there is any ambiguity, provide reasoning for your choice or ask a clarifying question. Ensure that each item listed is distinct and not a duplicate. After listing the items, count them explicitly and verify that the total matches the number of distinct items listed. Explicitly show all mathematical operations in your reasoning. For example, if you are adding quantities, write out the full addition process. Before providing the final answer, double-check that the total count is accurate and matches the items listed. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n","sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of each item and its relevance to the total count. Use bullet points or numbering with periods for listing items. Maintain consistent naming conventions for similar items (e.g., Bed 1, Bed 2). If there is any ambiguity, provide reasoning for your choice or ask a clarifying question. After listing the items, verify the total count to ensure accuracy. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"]},{"name":"stderr","output_type":"stream","text":["Accuracy: 0.8571428571428571: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1705.01it/s]\n","Accuracy: 0.7710843373493976: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:27<00:00,  3.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["val_performance:  0.7710843373493976\n","previous_performance:  0.891566265060241\n","rejected prompt: You will answer a reasoning question that involves counting items in a list. Provide a concise answer without detailed explanations. List each item clearly and sequentially without using bullet points or numbering. Use consistent naming conventions for similar items (e.g., Bed 1, Bed 2). If there is any ambiguity, make a reasonable assumption and provide the answer directly. Ensure the final line contains only the answer in the following format: 'Answer: $VALUE' where VALUE is a numerical value.\n","sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of each item and its relevance to the total count. Use bullet points or numbering with periods for listing items. Maintain consistent naming conventions for similar items (e.g., Bed 1, Bed 2). If there is any ambiguity, provide reasoning for your choice or ask a clarifying question. After listing the items, verify the total count to ensure accuracy. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"]},{"name":"stderr","output_type":"stream","text":["Accuracy: 0.8571428571428571: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1062.32it/s]\n","Training step 3. Epoch 2: : 3it [07:08, 142.76s/it]\n"]}],"source":["for epoch in range(3):\n","    for steps, (batch_x, batch_y) in enumerate((pbar := tqdm(train_loader, position=0))):\n","        pbar.set_description(f\"Training step {steps}. Epoch {epoch}\")\n","        optimizer.zero_grad()\n","        losses = []\n","        for (x, y) in zip(batch_x, batch_y):\n","            x = tg.Variable(x, requires_grad=False, role_description=\"query to the language model\")\n","            y = tg.Variable(y, requires_grad=False, role_description=\"correct answer for the query\")\n","            response = model(x)\n","            try:\n","                eval_output_variable = eval_fn(inputs=dict(prediction=response, ground_truth_answer=y))\n","            except:\n","                eval_output_variable = eval_fn([x, y, response])\n","            losses.append(eval_output_variable)\n","        total_loss = tg.sum(losses)\n","        total_loss.backward()\n","        optimizer.step()\n","\n","        run_validation_revert(system_prompt, results, model, eval_fn, val_set)\n","\n","        print(\"sys prompt: \", system_prompt)\n","        test_acc = eval_dataset(test_set, eval_fn, model)\n","        results[\"test_acc\"].append(test_acc)\n","        results[\"prompt\"].append(system_prompt.get_value())\n","        if steps == 3:\n","            break"]},{"cell_type":"code","execution_count":null,"id":"7dab7a53-e682-478e-9417-15009b495979","metadata":{"id":"7dab7a53-e682-478e-9417-15009b495979"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[{"file_id":"https://github.com/zou-group/TextGrad/blob/main/examples/notebooks/Tutorial-Prompt-Optimization.ipynb","timestamp":1737730487825}]},"widgets":{"application/vnd.jupyter.widget-state+json":{"b1cdd06b1a1e4413a7c6495d26bc8fe9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fa2ae6c3c1404138b47129e57e5569d5","IPY_MODEL_e2fb94b74a8e450faa62eb123d787ff7","IPY_MODEL_ac47320c1a9b4e0088143b4567c9e6b9"],"layout":"IPY_MODEL_33c930d93c4b44c5bf11e1bb42ca1111"}},"fa2ae6c3c1404138b47129e57e5569d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fcc8a2d9da74bc7a40ac095a476f91f","placeholder":"​","style":"IPY_MODEL_abee7f06559c456ba10aa9af5eca45f7","value":"README.md: 100%"}},"e2fb94b74a8e450faa62eb123d787ff7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_778cf62cd9a6423391b18d2b263c8fb3","max":99758,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e7e15ae3b5f84658b88fef91be2179ea","value":99758}},"ac47320c1a9b4e0088143b4567c9e6b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8579ea06a5844248591a5b2a68c645d","placeholder":"​","style":"IPY_MODEL_75139cbb0c0e467c825ebc6fda0c6c14","value":" 99.8k/99.8k [00:00&lt;00:00, 1.85MB/s]"}},"33c930d93c4b44c5bf11e1bb42ca1111":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fcc8a2d9da74bc7a40ac095a476f91f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abee7f06559c456ba10aa9af5eca45f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"778cf62cd9a6423391b18d2b263c8fb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7e15ae3b5f84658b88fef91be2179ea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c8579ea06a5844248591a5b2a68c645d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75139cbb0c0e467c825ebc6fda0c6c14":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de0b99a5bcad47f3a3f96b354d75182c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_17d8ce8b72bd4ecc917c5f94bc26855a","IPY_MODEL_d01bef9191e54155bc1d0f4a25d84671","IPY_MODEL_e0671a1ce02e48bc81eff4ea25676742"],"layout":"IPY_MODEL_2fece52c9bea46bf9672f499fab4d815"}},"17d8ce8b72bd4ecc917c5f94bc26855a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4316ad07dbe642748b9e741fe03b3e70","placeholder":"​","style":"IPY_MODEL_f4a960b954fc4b36a663d5d8ff692342","value":"bigbench.py: 100%"}},"d01bef9191e54155bc1d0f4a25d84671":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_00ca0ccc8b5c44d28465bd2917d0a46f","max":20544,"min":0,"orientation":"horizontal","style":"IPY_MODEL_08c80bd2ee6d4e818670fd90fc848dc6","value":20544}},"e0671a1ce02e48bc81eff4ea25676742":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b60ab69a03a54e6f8b1c784bf7cae130","placeholder":"​","style":"IPY_MODEL_31500b0fa96d48ad90636adda0a416f4","value":" 20.5k/20.5k [00:00&lt;00:00, 1.38MB/s]"}},"2fece52c9bea46bf9672f499fab4d815":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4316ad07dbe642748b9e741fe03b3e70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4a960b954fc4b36a663d5d8ff692342":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00ca0ccc8b5c44d28465bd2917d0a46f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08c80bd2ee6d4e818670fd90fc848dc6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b60ab69a03a54e6f8b1c784bf7cae130":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31500b0fa96d48ad90636adda0a416f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}